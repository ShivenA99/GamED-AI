# LLM-Driven Asset Generation Integration

## Overview

Yes! All the asset generation models can be **fully LLM-driven** through your existing agent pipeline. Your LLM agents generate prompts, and the asset models generate the actual assets.

## Current LLM-Driven Flow

### 1. Scene Generator Agent (Already LLM-Driven)
Your `scene_generator` agent already uses LLMs to:
- Generate asset descriptions
- Specify asset requirements
- Create asset prompts

**Example from your code:**
```python
# scene_generator_agent uses LLM to generate:
{
    "required_assets": [
        {
            "id": "sword_icon",
            "type": "image",
            "description": "Game icon, sword, fantasy style, clean background",
            "assetPrompt": "game icon, sword, fantasy style, clean background"  # ← LLM generates this!
        }
    ]
}
```

### 2. Blueprint Generator Agent (Already LLM-Driven)
Your `blueprint_generator` agent uses LLMs to:
- Create asset prompts in blueprints
- Specify asset requirements

**Example:**
```python
# blueprint_generator_agent creates:
{
    "diagram": {
        "assetPrompt": "educational diagram showing cell structure"  # ← LLM generates this!
    },
    "image": {
        "assetPrompt": "fantasy game background, medieval theme"  # ← LLM generates this!
    }
}
```

### 3. Asset Generator Agent (Needs Implementation)
Currently returns placeholders, but can be enhanced to:
- Take LLM-generated prompts from blueprint/scene
- Call asset generation models
- Return actual generated assets

---

## Integration Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    LLM Agents                          │
├─────────────────────────────────────────────────────────┤
│  Scene Generator (LLM)                                  │
│    ↓ Generates asset descriptions & prompts            │
│                                                          │
│  Blueprint Generator (LLM)                              │
│    ↓ Creates assetPrompt fields                         │
│                                                          │
│  Asset Generator (LLM + Asset Models)                   │
│    ↓ Takes prompts → Calls asset models                 │
└─────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────┐
│              Asset Generation Models                    │
├─────────────────────────────────────────────────────────┤
│  Stable Diffusion 3.5 Turbo (MLX)                      │
│    Input: "game icon, sword, fantasy style"              │
│    Output: PNG/JPEG image                               │
│                                                          │
│  StarVector-1B (SVG)                                     │
│    Input: "SVG icon of a cell nucleus, simple style"    │
│    Output: SVG code                                     │
│                                                          │
│  OmniSVG-3B (Complex SVG)                               │
│    Input: "educational diagram component, cell membrane"│
│    Output: SVG code                                     │
│                                                          │
│  AnimateDiff (Animation)                                │
│    Input: "spinning loading icon, game style"           │
│    Output: Animated GIF                                 │
└─────────────────────────────────────────────────────────┘
```

---

## Implementation: LLM-Driven Asset Generator

### Enhanced Asset Generator Agent

```python
# backend/app/agents/asset_generator.py

async def asset_generator_agent(state: AgentState, ctx=None) -> dict:
    """
    Generate assets using LLM-generated prompts + asset models.
    
    Flow:
    1. Extract asset prompts from blueprint/scene (generated by LLM agents)
    2. Use LLM to refine/enhance prompts if needed
    3. Call appropriate asset generation model
    4. Return generated asset URLs
    """
    from app.services.llm_service import get_llm_service
    from app.services.icon_generation_service import IconGenerationService
    from app.services.svg_generation_service import SVGGenerationService
    
    blueprint = state.get("blueprint", {})
    scene_data = state.get("scene_data", {})
    asset_urls = {}
    
    # Get LLM service for prompt refinement
    llm = get_llm_service()
    
    # Get asset generation services
    icon_service = IconGenerationService()
    svg_service = SVGGenerationService()
    
    # 1. Process diagram assets (from blueprint)
    if "diagram" in blueprint:
        diagram_prompt = blueprint["diagram"].get("assetPrompt")
        if diagram_prompt:
            # LLM can refine the prompt for better results
            refined_prompt = await llm.generate(
                f"Refine this asset generation prompt for best results: {diagram_prompt}"
            )
            
            # Generate SVG component using StarVector
            svg_code = await svg_service.generate_svg(refined_prompt)
            asset_urls["diagram"] = f"data:image/svg+xml;utf8,{quote(svg_code)}"
    
    # 2. Process image assets (from blueprint)
    if "image" in blueprint:
        image_prompt = blueprint["image"].get("assetPrompt")
        if image_prompt:
            # LLM can enhance the prompt
            enhanced_prompt = await llm.generate(
                f"Enhance this game asset prompt for Stable Diffusion: {image_prompt}. "
                f"Make it specific, include style, background, and quality keywords."
            )
            
            # Generate icon using Stable Diffusion
            icon_image = await icon_service.generate_icon(enhanced_prompt)
            asset_urls["image"] = save_and_get_url(icon_image)
    
    # 3. Process assets from scene_data
    required_assets = scene_data.get("required_assets", [])
    for asset in required_assets:
        asset_id = asset.get("id")
        asset_type = asset.get("type")
        description = asset.get("description")
        
        if asset_type == "image" and description:
            # LLM generates optimized prompt from description
            prompt = await llm.generate(
                f"Convert this asset description into an optimized Stable Diffusion prompt: {description}"
            )
            
            # Generate asset
            generated_asset = await icon_service.generate_icon(prompt)
            asset_urls[asset_id] = save_and_get_url(generated_asset)
        
        elif asset_type == "component" and "icon" in description.lower():
            # Generate SVG icon
            prompt = await llm.generate(
                f"Convert this component description into an SVG generation prompt: {description}"
            )
            
            svg_code = await svg_service.generate_svg(prompt)
            asset_urls[asset_id] = f"data:image/svg+xml;utf8,{quote(svg_code)}"
    
    return {
        **state,
        "asset_urls": asset_urls,
        "generation_complete": True
    }
```

---

## LLM Prompt Enhancement Examples

### Example 1: Icon Generation

**Initial Prompt (from Scene Generator LLM):**
```
"sword icon for game"
```

**Enhanced Prompt (by Asset Generator LLM):**
```
"game icon, sword, fantasy style, clean white background, 
high quality, detailed, 512x512, game asset style"
```

**Result:** Better Stable Diffusion output

### Example 2: SVG Generation

**Initial Description (from Scene Generator LLM):**
```
"icon of a cell nucleus"
```

**SVG Prompt (by Asset Generator LLM):**
```
"SVG icon of a cell nucleus, simple style, minimal design, 
educational diagram component, vector graphics, clean lines"
```

**Result:** Better StarVector output

### Example 3: Animation

**Initial Description (from Scene Generator LLM):**
```
"loading animation"
```

**Animation Prompt (by Asset Generator LLM):**
```
"spinning loading icon, game style, smooth animation, 
circular motion, 16 frames, clean design"
```

**Result:** Better AnimateDiff output

---

## Two-Level LLM Control

### Level 1: High-Level Planning (Existing)
- **Scene Generator**: Plans what assets are needed
- **Blueprint Generator**: Specifies asset requirements
- **Uses**: Your existing LLM agents (Qwen2.5 7B, etc.)

### Level 2: Prompt Optimization (New)
- **Asset Generator**: Refines prompts for asset models
- **Uses**: Same LLM agents to enhance prompts
- **Benefit**: Better asset generation results

---

## Which Models Are LLMs?

### Built on LLMs (Vision-Language Models):
1. **OmniSVG** - Built on Qwen-VL (Vision-Language Model)
2. **StarVector** - Vision-language architecture
3. **Both accept text prompts** - Can be driven by your LLM agents

### Not LLMs (But LLM-Controllable):
1. **Stable Diffusion** - Diffusion model, accepts text prompts
2. **AnimateDiff** - Motion module, works with text prompts
3. **Both accept text prompts** - Can be driven by your LLM agents

**Key Point:** Even non-LLM models are **LLM-driven** because they accept text prompts that your LLM agents generate!

---

## Complete Integration Flow

```
1. Input Enhancer (LLM)
   ↓ Extracts pedagogical context

2. Scene Generator (LLM)
   ↓ Generates asset descriptions
   "sword icon, fantasy style"

3. Blueprint Generator (LLM)
   ↓ Creates assetPrompt fields
   "game icon, sword, fantasy style, clean background"

4. Asset Generator (LLM + Models)
   ↓ LLM refines prompt
   "game icon, sword, fantasy style, clean white background, 
    high quality, detailed, 512x512, game asset style"
   ↓ Calls Stable Diffusion 3.5 Turbo (MLX)
   ↓ Generates actual icon image
   ↓ Returns asset URL

5. Game Output
   ↓ Uses generated assets
```

---

## Benefits of LLM-Driven Approach

1. **Intelligent Prompting**: LLMs understand context and generate better prompts
2. **Context Awareness**: Assets match pedagogical goals and game themes
3. **Adaptive**: LLMs can refine prompts based on previous results
4. **Consistent Style**: LLMs can maintain style consistency across assets
5. **Error Recovery**: LLMs can adjust prompts if generation fails

---

## Example: Full LLM-Driven Asset Generation

```python
# Scene Generator (LLM) creates:
scene_data = {
    "required_assets": [
        {
            "id": "sword_icon",
            "description": "fantasy sword icon for inventory",
            "type": "image"
        }
    ]
}

# Blueprint Generator (LLM) creates:
blueprint = {
    "image": {
        "assetPrompt": "game icon, sword, fantasy style"
    }
}

# Asset Generator (LLM + Model):
# 1. LLM refines prompt
refined = await llm.generate(
    "Enhance this game asset prompt: 'game icon, sword, fantasy style'"
)
# Result: "game icon, sword, fantasy style, clean white background, 
#          high quality, detailed, 512x512, game asset style"

# 2. Call Stable Diffusion
icon = await icon_service.generate_icon(refined)

# 3. Return asset
asset_urls["sword_icon"] = save_and_get_url(icon)
```

---

## Summary

**Yes, all asset generation models are LLM-driven:**

1. ✅ **Your LLM agents generate prompts** (Scene Generator, Blueprint Generator)
2. ✅ **LLM can refine prompts** (Asset Generator enhancement)
3. ✅ **Asset models execute prompts** (Stable Diffusion, StarVector, etc.)
4. ✅ **Full integration** with your existing agent pipeline

**Next Steps:**
1. Implement asset generation services (Icon, SVG, Animation)
2. Enhance Asset Generator agent to use these services
3. Add LLM prompt refinement for better results
4. Test with your existing LLM-driven pipeline

The models themselves may not be LLMs, but they're **fully controlled by your LLM agents** through text prompts!
